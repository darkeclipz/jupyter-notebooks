{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage would be to being able to solve algebraic expressions with only operations and constants.\n",
    "\n",
    "For example, solve: $\\dfrac{1}{2}\\cdot3+5\\cdot(2\\cdot7^3)+(-1)^2$.\n",
    "\n",
    "In later stages it might be desirable to solve equalities, inequalities, and express in terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to support the following:\n",
    "\n",
    "* Addition/subtraction\n",
    "* Multiplication/division (also implicit multiplication)\n",
    "* Parenthesis\n",
    "* Exponents\n",
    "* Square root\n",
    "* Trigonometric functions\n",
    "* Logarithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should follow PEMDAS:\n",
    "\n",
    "1. Parenthesis\n",
    "2. Exponents\n",
    "3. Multiplication/Division\n",
    "4. Addition/Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will hold the type of token and the value of the token. There are different types of tokens:\n",
    "\n",
    "**Types:** `operator`, `function`, `term`, `constant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"['%s': '%s']\" % (self.type, self.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is engine for the parser, all the character/digits will be converted into a list of tokens which can be used in lexical analysis later on.\n",
    "\n",
    "**Whitespace characters:** `<space>`, `\\n`, `\\r`, `\\t`.\n",
    "\n",
    "**Supported operators:** `+`, `-`, `*`, `/`, `=`, `^`, `(`, `)`, `,`.\n",
    "\n",
    "**Supported functions:** `sqrt`, `sin`, `asin`, `cos`, `acos`, `tan`, `atan`, `log`, `ln`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "    whitespace = [' ', '\\r', '\\n', '\\t']\n",
    "    operators = {'+':'add', '-':'subtract', '*':'multiply', '/':'divide', '=':'equal',\n",
    "               '^':'power', '(':'left-parenthesis', ')':'right-parenthesis', ',':'comma'}\n",
    "    functions = ['sqrt', 'sin', 'asin', 'cos', 'acos', 'tan', 'atan', 'log', 'ln']\n",
    "    \n",
    "    def __init__(self, expression):\n",
    "        self.expression = expression\n",
    "        self.index = 0\n",
    "        self.tokens = []\n",
    "        self.tokenize()\n",
    "        \n",
    "    def next(self, skip = 1):\n",
    "        self.index += skip\n",
    "        \n",
    "    def end(self):\n",
    "        return self.index >= len(self.expression)\n",
    "    \n",
    "    def nextCharacter(self):\n",
    "        return self.expression[self.index:self.index+1]\n",
    "    \n",
    "    def eatWhitespace(self):\n",
    "        while self.nextCharacter() in self.whitespace:\n",
    "            self.next()\n",
    "            \n",
    "    def eatDigit(self):\n",
    "        digit = ''\n",
    "        while self.nextCharacter().isdigit() or self.nextCharacter() == '.':\n",
    "            digit += self.nextCharacter()\n",
    "            self.next()\n",
    "        return digit\n",
    "    \n",
    "    def eatWord(self):\n",
    "        word = ''\n",
    "        while self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "            word += self.nextCharacter()\n",
    "            self.next()\n",
    "        return word\n",
    "            \n",
    "    def tokenize(self):\n",
    "        while(not self.end()):\n",
    "            self.eatWhitespace()\n",
    "            if self.nextCharacter() in self.operators:\n",
    "                token = Token('operator', self.operators[self.nextCharacter()])\n",
    "                self.tokens.append(token)\n",
    "                self.next()\n",
    "            elif self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "                word = self.eatWord()\n",
    "                if word in self.functions:\n",
    "                    token = Token('function', word)\n",
    "                elif len(word) == 1:\n",
    "                    token = Token('term', word)\n",
    "                else:\n",
    "                    raise ValueError('Invalid character at index %i' % self.index)\n",
    "                self.tokens.append(token)\n",
    "            elif self.nextCharacter().isdigit():\n",
    "                digit = self.eatDigit()\n",
    "                token = Token('constant', digit)\n",
    "                self.tokens.append(token)\n",
    "            else:\n",
    "                raise ValueError('Invalid character at index %i' % self.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tokenizer with any mathematical expression, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('(2g + sqrt(36x) * ln(2) * acos(6)A * 2) / 3.14 tan(5) = log(23,5) + 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is example of the output of the tokenizer. The result is a list of `Token` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['operator': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['term': 'g'],\n",
       " ['operator': 'add'],\n",
       " ['function': 'sqrt'],\n",
       " ['operator': 'left-parenthesis'],\n",
       " ['constant': '36'],\n",
       " ['term': 'x'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['operator': 'multiply'],\n",
       " ['function': 'ln'],\n",
       " ['operator': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['operator': 'multiply'],\n",
       " ['function': 'acos'],\n",
       " ['operator': 'left-parenthesis'],\n",
       " ['constant': '6'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['term': 'A'],\n",
       " ['operator': 'multiply'],\n",
       " ['constant': '2'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['operator': 'divide'],\n",
       " ['constant': '3.14'],\n",
       " ['function': 'tan'],\n",
       " ['operator': 'left-parenthesis'],\n",
       " ['constant': '5'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['operator': 'equal'],\n",
       " ['function': 'log'],\n",
       " ['operator': 'left-parenthesis'],\n",
       " ['constant': '23'],\n",
       " ['operator': 'comma'],\n",
       " ['constant': '5'],\n",
       " ['operator': 'right-parenthesis'],\n",
       " ['operator': 'add'],\n",
       " ['constant': '3']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an expression tree from the tokens. We should be able to solve expressions with only constants and operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, left, right, operation):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.operation = operation\n",
    "        \n",
    "class Constant():\n",
    "    def __init__(self, value):\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExpressionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionTree():\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.stack = []\n",
    "        self.index = 0\n",
    "        self.parse()\n",
    "        \n",
    "    def next(self, skip = 1):\n",
    "        self.index += skip\n",
    "        \n",
    "    def peek(self):\n",
    "        return self.stack[-1]\n",
    "        \n",
    "    def end(self):\n",
    "        return self.index >= len(self.tokens)\n",
    "    \n",
    "    def nextToken(self):\n",
    "        return self.tokens[self.index:self.index+1][0]\n",
    "    \n",
    "    def findClosingParenthesisOffset(self):\n",
    "        parenthesis = 1\n",
    "        offset = 0\n",
    "        while parenthesis > 0:\n",
    "            if self.index+offset >= len(self.tokens):\n",
    "                return -1\n",
    "            if self.tokens[self.index+offset:self.index+offset+1][0].value == 'right-parenthesis':\n",
    "                parenthesis -= 1\n",
    "            elif self.tokens[self.index+offset:self.index+offset+1][0].value == 'left-parenthesis':\n",
    "                parenthesis += 1\n",
    "            offset += 1\n",
    "        return offset-1\n",
    "    \n",
    "    def parse(self):\n",
    "        iteration = 0\n",
    "        while(not self.end()):\n",
    "            token = self.nextToken()\n",
    "            print('Iteration %i, Stack: %s, Token: %s' % (iteration, self.stack, token))\n",
    "            if token.type == 'constant':\n",
    "                if len(self.stack) > 0 and self.peek().type == 'operator':\n",
    "                    operation = self.stack.pop()\n",
    "                    ltoken = self.stack.pop()\n",
    "                    if operation.value == 'add':\n",
    "                        self.stack.append(Token('constant', int(ltoken.value) + int(token.value)))\n",
    "                    elif operation.value == 'multiply':\n",
    "                        self.stack.append(Token('constant', int(ltoken.value) * int(token.value)))\n",
    "                    elif operation.value == 'left-parenthesis':\n",
    "                        offset = self.findClosingParenthesisOffset()\n",
    "                        print(offset+self.index)\n",
    "                else:\n",
    "                    self.stack.append(token)\n",
    "            elif token.type == 'operator':\n",
    "                self.stack.append(token)\n",
    "            elif token.type == 'function':\n",
    "                raise ValueError('Function at index %i is not supported.' % self.index)\n",
    "            elif token.type == 'term':\n",
    "                raise ValueError('Term at index %i is not supported.' % self.index)\n",
    "            else:\n",
    "                raise ValueError('Unrecognized token type: %s' % token.type)\n",
    "            self.next()\n",
    "            iteration+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to parse the tokens into an expression tree, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Stack: [], Token: ['constant': '1']\n",
      "Iteration 1, Stack: [['constant': '1']], Token: ['operator': 'add']\n",
      "Iteration 2, Stack: [['constant': '1'], ['operator': 'add']], Token: ['constant': '2']\n",
      "Iteration 3, Stack: [['constant': '3']], Token: ['operator': 'multiply']\n",
      "Iteration 4, Stack: [['constant': '3'], ['operator': 'multiply']], Token: ['constant': '5']\n",
      "The result is: 15 \n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer('1+2*5')\n",
    "expressionTree = ExpressionTree(tokenizer.tokens)\n",
    "print('The result is: %i ' % expressionTree.stack[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issues / current state**\n",
    "\n",
    "The current implementation doesn't account for the order of operations. It should not solve the equation, but build an expression tree instead. This was implemented as a test. Also, the parenthesis functionality is not tested, the idea was to recursively call the `ExpressionTree` for each set of a parentheses and push the output to the stack, or created a simplified token list (preferred). \n",
    "\n",
    "Finally, some sort of state machine should cycle through the order of operations and simplify the equation accordingly. In theory it should be easy to solve it this way.\n",
    "\n",
    "Other goal: try to solve the equations with a `Stack` structure, there is probably a method but I don't want to Google that yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase we are going to verify if the entered syntax is valid. Things we check in this phase are:\n",
    "\n",
    "* Is there more than $1$ `=` sign in the expression?\n",
    "* Does every function get the right amount of arguments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexicalAnalysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalAnalysis():\n",
    "    def __init__(self, expressionTree):\n",
    "        self.expressionTree = expressionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
