{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve `1/2*10*(5+2)/3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should follow PEMDAS:\n",
    "\n",
    "1. Parenthesis\n",
    "2. Exponents\n",
    "3. Multiplication/Division\n",
    "4. Addition/Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing (Tokenizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will hold the type of token and the value of the token. There are different types of tokens:\n",
    "\n",
    "**Types:** `symbol`, `function`, `term`, `constant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"['%s': '%s']\" % (self.type, self.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is engine for the parser, all the character/digits will be converted into a list of tokens which can be used in lexical analysis later on.\n",
    "\n",
    "**Whitespace characters:** `<space>`, `\\n`, `\\r`, `\\t\\`.\n",
    "\n",
    "**Supported symbols:** `+`, `-`, `*`, `/`, `=`, `^`, `(`, `)`, `,`.\n",
    "\n",
    "**Supported functions:** `sin`, `asin`, `cos`, `acos`, `tan`, `atan`, `log`, `ln`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "    whitespace = [' ', '\\r', '\\n', '\\t']\n",
    "    symbols = {'+':'add', '-':'subtract', '*':'multiply', '/':'divide', '=':'equal',\n",
    "               '^':'power', '(':'left-parenthesis', ')':'right-parenthesis', ',':'comma'}\n",
    "    functions = ['sin', 'asin', 'cos', 'acos', 'tan', 'atan', 'log', 'ln']\n",
    "    \n",
    "    def __init__(self, expression):\n",
    "        self.expression = expression\n",
    "        self.index = 0\n",
    "        self.tokens = []\n",
    "        self.tokenize()\n",
    "        \n",
    "    def next(self, skip = 1):\n",
    "        self.index += skip\n",
    "        \n",
    "    def end(self):\n",
    "        if self.index >= len(self.expression): return True\n",
    "        return False\n",
    "    \n",
    "    def nextCharacter(self):\n",
    "        return self.expression[self.index:self.index+1]\n",
    "    \n",
    "    def eatWhitespace(self):\n",
    "        while self.nextCharacter() in self.whitespace:\n",
    "            self.next()\n",
    "            \n",
    "    def eatDigit(self):\n",
    "        digit = ''\n",
    "        while self.nextCharacter().isdigit():\n",
    "            digit += self.nextCharacter()\n",
    "            self.next()\n",
    "        return digit\n",
    "    \n",
    "    def eatWord(self):\n",
    "        word = ''\n",
    "        while self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "            word += self.nextCharacter()\n",
    "            self.next()\n",
    "        return word\n",
    "            \n",
    "    def tokenize(self):\n",
    "        while(not self.end()):\n",
    "            self.eatWhitespace()\n",
    "            if self.nextCharacter() in self.symbols:\n",
    "                token = Token('symbol', self.symbols[self.nextCharacter()])\n",
    "                self.tokens.append(token)\n",
    "                self.next()\n",
    "            elif self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "                word = self.eatWord()\n",
    "                if word in self.functions:\n",
    "                    token = Token('function', word)\n",
    "                elif len(word) == 1:\n",
    "                    token = Token('term', word)\n",
    "                else:\n",
    "                    raise ValueError('Invalid character at index %i near %s'  % (self.index, self.expression[self.index-5:self.index+5]))\n",
    "                self.tokens.append(token)\n",
    "            elif self.nextCharacter().isdigit():\n",
    "                digit = self.eatDigit()\n",
    "                token = Token('constant', digit)\n",
    "                self.tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tokenizer with any mathematical expression, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('(2g + 36x * ln(2) * acos(6)A * 2) / 2 tan(5) = log(23,5) + 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is example of the output of the tokenizer. The result is a list of `Token` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['symbol': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['term': 'g'],\n",
       " ['symbol': 'add'],\n",
       " ['constant': '36'],\n",
       " ['term': 'x'],\n",
       " ['symbol': 'multiply'],\n",
       " ['function': 'ln'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'multiply'],\n",
       " ['function': 'acos'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '6'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['term': 'A'],\n",
       " ['symbol': 'multiply'],\n",
       " ['constant': '2'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'divide'],\n",
       " ['constant': '2'],\n",
       " ['function': 'tan'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '5'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'equal'],\n",
       " ['function': 'log'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '23'],\n",
       " ['symbol': 'comma'],\n",
       " ['constant': '5'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'add'],\n",
       " ['constant': '3']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
