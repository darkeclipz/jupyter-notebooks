{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage would be to being able to solve algebraic expressions with only operations and constants.\n",
    "\n",
    "For example, solve: $\\dfrac{1}{2}\\cdot3+5\\cdot(2\\cdot7^3)+(-1)^2$.\n",
    "\n",
    "In later stages it might be desirable to solve equalities, inequalities, and express in terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to support the following:\n",
    "\n",
    "* Addition/subtraction\n",
    "* Multiplication/division (also implicit multiplication)\n",
    "* Parenthesis\n",
    "* Exponents\n",
    "* Square root\n",
    "* Trigonometric functions\n",
    "* Logarithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should follow PEMDAS:\n",
    "\n",
    "1. Parenthesis\n",
    "2. Exponents\n",
    "3. Multiplication/Division\n",
    "4. Addition/Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will hold the type of token and the value of the token. There are different types of tokens:\n",
    "\n",
    "**Types:** `symbol`, `function`, `term`, `constant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"['%s': '%s']\" % (self.type, self.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is engine for the parser, all the character/digits will be converted into a list of tokens which can be used in lexical analysis later on.\n",
    "\n",
    "**Whitespace characters:** `<space>`, `\\n`, `\\r`, `\\t`.\n",
    "\n",
    "**Supported symbols:** `+`, `-`, `*`, `/`, `=`, `^`, `(`, `)`, `,`.\n",
    "\n",
    "**Supported functions:** `sqrt`, `sin`, `asin`, `cos`, `acos`, `tan`, `atan`, `log`, `ln`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer():\n",
    "    whitespace = [' ', '\\r', '\\n', '\\t']\n",
    "    symbols = {'+':'add', '-':'subtract', '*':'multiply', '/':'divide', '=':'equal',\n",
    "               '^':'power', '(':'left-parenthesis', ')':'right-parenthesis', ',':'comma'}\n",
    "    functions = ['sqrt', 'sin', 'asin', 'cos', 'acos', 'tan', 'atan', 'log', 'ln']\n",
    "    \n",
    "    def __init__(self, expression):\n",
    "        self.expression = expression\n",
    "        self.index = 0\n",
    "        self.tokens = []\n",
    "        self.tokenize()\n",
    "        \n",
    "    def next(self, skip = 1):\n",
    "        self.index += skip\n",
    "        \n",
    "    def end(self):\n",
    "        return self.index >= len(self.expression)\n",
    "    \n",
    "    def nextCharacter(self):\n",
    "        return self.expression[self.index:self.index+1]\n",
    "    \n",
    "    def eatWhitespace(self):\n",
    "        while self.nextCharacter() in self.whitespace:\n",
    "            self.next()\n",
    "            \n",
    "    def eatDigit(self):\n",
    "        digit = ''\n",
    "        while self.nextCharacter().isdigit() or self.nextCharacter() == '.':\n",
    "            digit += self.nextCharacter()\n",
    "            self.next()\n",
    "        return digit\n",
    "    \n",
    "    def eatWord(self):\n",
    "        word = ''\n",
    "        while self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "            word += self.nextCharacter()\n",
    "            self.next()\n",
    "        return word\n",
    "            \n",
    "    def tokenize(self):\n",
    "        while(not self.end()):\n",
    "            self.eatWhitespace()\n",
    "            if self.nextCharacter() in self.symbols:\n",
    "                token = Token('symbol', self.symbols[self.nextCharacter()])\n",
    "                self.tokens.append(token)\n",
    "                self.next()\n",
    "            elif self.nextCharacter() in string.ascii_lowercase + string.ascii_uppercase:\n",
    "                word = self.eatWord()\n",
    "                if word in self.functions:\n",
    "                    token = Token('function', word)\n",
    "                elif len(word) == 1:\n",
    "                    token = Token('term', word)\n",
    "                else:\n",
    "                    raise ValueError('Invalid character at index %i' % self.index)\n",
    "                self.tokens.append(token)\n",
    "            elif self.nextCharacter().isdigit():\n",
    "                digit = self.eatDigit()\n",
    "                token = Token('constant', digit)\n",
    "                self.tokens.append(token)\n",
    "            else:\n",
    "                raise ValueError('Invalid character at index %i' % self.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tokenizer with any mathematical expression, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('(2g + sqrt(36x) * ln(2) * acos(6)A * 2) / 3.14 tan(5) = log(23,5) + 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is example of the output of the tokenizer. The result is a list of `Token` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['symbol': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['term': 'g'],\n",
       " ['symbol': 'add'],\n",
       " ['function': 'sqrt'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '36'],\n",
       " ['term': 'x'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'multiply'],\n",
       " ['function': 'ln'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '2'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'multiply'],\n",
       " ['function': 'acos'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '6'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['term': 'A'],\n",
       " ['symbol': 'multiply'],\n",
       " ['constant': '2'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'divide'],\n",
       " ['constant': '3.14'],\n",
       " ['function': 'tan'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '5'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'equal'],\n",
       " ['function': 'log'],\n",
       " ['symbol': 'left-parenthesis'],\n",
       " ['constant': '23'],\n",
       " ['symbol': 'comma'],\n",
       " ['constant': '5'],\n",
       " ['symbol': 'right-parenthesis'],\n",
       " ['symbol': 'add'],\n",
       " ['constant': '3']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an expression tree from the tokens. We should be able to solve expressions with only constants and operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, left, right, operation):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.operation = operation\n",
    "        \n",
    "class Constant():\n",
    "    def __init__(self, value):\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExpressionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionTree():\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.stack = []\n",
    "        self.index = 0\n",
    "        self.parse()\n",
    "        \n",
    "    def next(self, skip = 1):\n",
    "        self.index += skip\n",
    "        \n",
    "    def peek(self):\n",
    "        return self.stack.last\n",
    "        \n",
    "    def end(self):\n",
    "        return self.index >= len(self.tokens)\n",
    "    \n",
    "    def nextToken(self):\n",
    "        return self.tokens[self.index:self.index+1]\n",
    "    \n",
    "    def parse(self):\n",
    "        while(not self.end()):\n",
    "            token = self.nextToken()\n",
    "            if token.type == 'constant':\n",
    "                \n",
    "                self.stack.push(token)\n",
    "            elif token.type == 'symbol':\n",
    "                \n",
    "            elif token.type == 'function':\n",
    "            elif token.type == 'term':\n",
    "            else:\n",
    "                raise ValueError('Unrecognized token type: %s' % token.type)\n",
    "            self.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to parse the tokens into an expression tree, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['constant': '2']]\n",
      "[['symbol': 'add']]\n",
      "[['symbol': 'left-parenthesis']]\n",
      "[['constant': '5']]\n",
      "[['symbol': 'add']]\n",
      "[['constant': '3']]\n",
      "[['symbol': 'right-parenthesis']]\n",
      "[['symbol': 'multiply']]\n",
      "[['constant': '3']]\n",
      "[['symbol': 'divide']]\n",
      "[['symbol': 'left-parenthesis']]\n",
      "[['constant': '1']]\n",
      "[['symbol': 'add']]\n",
      "[['constant': '1']]\n",
      "[['symbol': 'right-parenthesis']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer('2+(5+3)*3/(1+1)')\n",
    "expressionTree = ExpressionTree(tokenizer.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase we are going to verify if the entered syntax is valid. Things we check in this phase are:\n",
    "\n",
    "* Is there more than $1$ `=` sign in the expression?\n",
    "* Does every function get the right amount of arguments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexicalAnalysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalAnalysis():\n",
    "    def __init__(self, expressionTree):\n",
    "        self.expressionTree = expressionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
